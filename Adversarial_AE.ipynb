{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adversarial_AE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVCV96E-p2gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyhC_471rrAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_Fashion/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWZj4oofsHy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0002\n",
        "batch_size = 32\n",
        "epochs = 100000\n",
        "\n",
        "image_dimension = 784\n",
        "neural_network_dimension = 128\n",
        "latent_variable_dimension = 10\n",
        "z_noise_dim = 10\n",
        "\n",
        "def xavier(in_shape):\n",
        "  val = tf.random_normal(shape = in_shape, stddev = 1./tf.sqrt(in_shape[0]/2.))\n",
        "  return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqGIoPZusegL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Disc_W = {\"disc_H\" : tf.Variable(xavier_init([latent_variable_dimension, neural_network_dimension])),\n",
        "          \"disc_final\" : tf.Variable(xavier_init([neural_network_dimension, 1]))\n",
        "          }\n",
        "\n",
        "Disc_ Bias = {\"disc_H\" : tf.Variable(xavier_init([neural_network_dimension])),\n",
        "          \"disc_final\" : tf.Variable(xavier_init([1]))\n",
        "          }\n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkHnBkN6tOe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AE_weight = {\"E_weight_encoder\" : tf.Variable(xavier_init([image_dimension, neural_network_dimension])),\n",
        "             \"E_weight_hidden\" : tf.Variable(xavier_init([neural_network_dimension, latent_variable_dimension])),\n",
        "\n",
        "\n",
        "             \"D_weight_hidden\" : tf.Variable(xavier_init([latent_variable_dimension, neural_network_dimension])),\n",
        "             \"D_weight_decoder\" : tf.Variable(xavier_init([neural_network_dimension, image_dimension]))\n",
        "          }\n",
        "\n",
        "AE_bias = {\"E_bias_encoder\" : tf.Variable(xavier_init([neural_network_dimension])),\n",
        "           \"E_bias_hidden\" : tf.Variable(xavier_init([latent_variable_dimension])),\n",
        "\n",
        "           \"D_bias_hidden\" : tf.Variable(xavier_init([neural_network_dimension])),\n",
        "           \"D_bias_decoder\" : tf.Variable(xavier_init([image_dimension]))\n",
        "          }          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItWRFe9eubw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#placeholders for the inputs\n",
        "\n",
        "Z_input = tf.placeholder(tf.float32, shape = [None, z_noise_dim], name = \"input_noise\")\n",
        "X_input = tf.placeholder(tf.float32, shape = [None, image_dimension], name = \"real_input\")\n",
        "\n",
        "def Encoder(x):\n",
        "  hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, AE_weight['E_weight_encoder']), AE_bias['E_bias_encoder']))\n",
        "  encoder_output = tf.add(tf.matmul(hidden_layer, AE_weight['E_weight_hidden']), AE_bias['E_bias_hidden'])\n",
        "  return encoder_output\n",
        "\n",
        "\n",
        "def Decoder(x):\n",
        "  hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, AE_weight['D_weight_hidden']), AE_bias['D_bias_hidden']))\n",
        "  decoder_output = tf.add(tf.matmul(hidden_layer, AE_weight['D_weight_decoder']), AE_bias['D_bias_decoder'])\n",
        "  prob = tf.nn.sigmoid(decoder_output)\n",
        "  return prob, decoder_output\n",
        "\n",
        "\n",
        "def Discriminator(x):\n",
        "  hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, Disc_W['disc_H']), Disc_Bias['disc_H']))\n",
        "  final_output = tf.add(tf.matmul(hidden_layer, Disc_W['disc_final']), Disc_Bias['disc_final'])\n",
        "  disc_op = tf.nn.sigmoid(final_output)\n",
        "  return disc_op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQvleeB_xj8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the graph\n",
        "\n",
        "latent_output = Encoder(X_input)\n",
        "_, final_output = Decoder(latent_output)\n",
        "\n",
        "real_output_Disc = Discriminator(Z_input) #prior gaussian, bernoulli or uniform distribution\n",
        "fake_output_Disc = Discriminator(latent_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zImKLg1AyCPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AE_Loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = final_output, labels = X_input))\n",
        "\n",
        "Discriminator_Loss = -tf.reduce_mean(tf.log(real_output_Disc) + tf.log(1.0 - fake_output_Disc))\n",
        "Generator_Loss = -tf.reduce_mean(tf.log(fake_output_Disc))\n",
        "\n",
        "#tensorboard summary\n",
        "tf.summary.scalar(\"AE_loss\", AE_Loss)\n",
        "tf.summary.scalar(\"Disc_Total_loss\", Discriminator_Loss)\n",
        "tf.summary.scalar(\"Gen_loss\", Generator_Loss)\n",
        "tf.summary.histogram(\"Gen_loss\", latent_output)\n",
        "\n",
        "summary1 = tf.summary.merge_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lupoAYEOz_Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Encoder_var = [AE_weight['E_weight_encoder'], AE_bias['E_bias_encoder'], AE_weight['E_weight_hidden'], AE_bias['E_bias_hidden']]\n",
        "Decoder_var = [AE_weight['D_weight_hidden'], AE_bias['D_bias_hidden'], AE_weight['D_weight_decoder'],  AE_bias['D_bias_decoder']]\n",
        "Discriminator_var = [Disc_W['disc_H'], Disc_Bias['disc_H'], Disc_W['disc_final'], Disc_Bias['disc_final']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_PAlWlE0uIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AutoEncoder_optimize = tf.train.AdamOptimizer(learning_rate = lr).minimize(AE_loss, var_list = Encoder_var + Decoder_var)\n",
        "Discriminator_optimize = tf.train.AdamOptimizer(learning_rate = lr).minimize(Discriminator_loss, var_list = Discriminator_var)\n",
        "Generator_optimize = tf.train.AdamOptimizer(learning_rate = lr).minimize(Generator_loss, var_list = Encoder_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPQupP8D1cB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
        "\n",
        "for i in range(epochs):\n",
        "  X_batch, _ = mnist.train.next_batch(batch_size)\n",
        "  Z_noise = np.random.normal(0, 1, [batch_size, z_noise_dim])\n",
        "\n",
        "  _, AE_loss_epoch = sess.run([AutoEncoder_optimize, AE_Loss], feed_dict = {X_input: X_batch})\n",
        "  _, Disc_loss_epoch = sess.run([Discriminator_optimize, Discriminator_Loss], feed_dict = {X_input: X_batch, Z_input : Z_noise})\n",
        "  _, Gen_loss_epoch = sess.run([Generator_optimize, Generator_Loss], feed_dict = {X_input: X_batch})\n",
        "\n",
        "  writer.add_summary(summary.epoch)\n",
        "\n",
        "if i%2000 == 0:\n",
        "    print('Steps: {0}, AE Loss: {1}, Generator Loss: {2}, Discriminator Loss: {3}'.format(i, AE_loss_epoch, Gen_loss_epoch, Disc_loss_epoch))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mfzow2_3ZFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualising the output obtained ----\n",
        "\n",
        "test_output , _ = Decoder(Z_input)\n",
        "n = 6\n",
        "canvas = np.empty((28*n, 28*n))\n",
        "\n",
        "for i in range(n):\n",
        "  Z_noise = np.random.normal(0, 1, [batch_size, z_noise_dim])\n",
        "    #generated_latent_layer = np.random.normal(0, 1, size = [batch_size, latent_variable_dimension])\n",
        "  generated_image  = sess.run(test_output, feed_dict = {Z_input : Z_noise})\n",
        "  for j in range(n):\n",
        "    canvas[i * 28: (i+1) * 28, j * 28: (j+1) * 28] = generated_image[j].reshape([28, 28])\n",
        "\n",
        "\n",
        "plt.figure(figsize = (n, n))\n",
        "plt.imshow(canvas, origin = \"upper\", cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ3eJtcc4Wwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in the log files go and check the tensorboard logs and open the needed link in browser\n",
        "# u can view each of the losses and also histogram of the approximated latent space, here a normal dist"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}