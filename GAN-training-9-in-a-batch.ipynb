{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"conda install -c conda-forge gdown","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gdown\nurl = 'https://drive.google.com/uc?id=1rcz0ZRJ0uLoEucq8mposcsUOqFn202bf'\noutput = 'file.mat'\ngdown.download(url, output, quiet=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.io import loadmat\nannots = loadmat('file.mat')\ndata = [item[0:] for item in annots['h']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport os\nimport tensorflow as tf\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import rand\nfrom numpy.random import randn\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Conv1D\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Input\nfrom keras.layers import Embedding, BatchNormalization, LayerNormalization\nimport tensorflow_addons as tfa\nfrom keras.layers import Concatenate, LeakyReLU\nfrom keras.layers import Reshape\nfrom keras.utils.vis_utils import plot_model\nfrom numpy.random import randint\nimport numpy as np\nimport time\n#from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_array = np.array(data)\ndata_env1 = data_array[:, :, 0]\ndataset = []\nnum_angles = 90\nangle = 0\nfor angle in range(num_angles):\n    angle_i_data = data_env1[:, angle]\n    dataset.append(angle_i_data)\ndataset = np.array(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_discriminator(latent_dim,n_classes = 90):\n    # label input\n    in_label = Input(shape=(1,))\n    #embedding for categorical input\n    li = Embedding(n_classes , 25)(in_label)\n    #linear multiplication\n    n_nodes = 1\n    li = Dense(n_nodes)(li)\n    li = Reshape((n_nodes, 1))(li)\n    in_dim = Input(shape=(latent_dim, 1))\n    merge = Concatenate()([in_dim,li])\n    merge = Conv1D(filters=256, kernel_size=5, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(64, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=128, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(64, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=64, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(64, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=64, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(64, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=64, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(64, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=16, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(100, activation='relu')(merge)\n    out_layer = Dense(1, activation='sigmoid')(merge)\n    model = Model([in_dim, in_label], out_layer)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    #model.compile(loss='mean_absolute_percentage_error', optimizer=tf.keras.optimizers.Adam(lr=1e-4, beta_1 = 0.901), metrics=['accuracy'])\n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1 = 0.901), metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_generator(latent_dim_gen, n_classes = 90, n_outputs = 1):\n    # label input\n    in_label = Input(shape=(1,))\n    # embedding for categorical input\n    li = Embedding(n_classes ,25)(in_label)\n    # linear multiplication\n    n_nodes = 15\n    li = Dense(n_nodes)(li)\n    # image generator input\n    in_lat = Input(shape=(latent_dim_gen,))\n    gen = Dense(n_nodes, activation = 'relu',  kernel_initializer='he_uniform')(in_lat)\n    gen = Reshape((1,15))(gen)\n    merge = Concatenate()([gen,li])\n    merge = Conv1D(filters=256, kernel_size=5, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(32, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=128, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(32, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=64, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    merge = Dense(32, activation = 'relu',  kernel_initializer='he_uniform')(merge)\n    merge = Conv1D(filters=16, kernel_size=3, padding='same')(merge)\n    merge = BatchNormalization()(merge, training=True)\n    merge = tf.nn.relu(merge)\n    out_layer = Dense(n_outputs, activation='linear')(merge)\n    model = Model([in_lat, in_label], out_layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GAN_model(generator, discriminator):\n    discriminator.trainable = False\n    gen_noise, gen_label = generator.input\n    gen_output = generator.output\n    gan_output = discriminator([gen_output, gen_label])\n    model = Model([gen_noise, gen_label], gan_output)\n    #model.compile(loss='mean_absolute_percentage_error', optimizer=tf.keras.optimizers.Adam(lr=1e-4, beta_1 = 0.901), metrics=['accuracy'])\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.7015)\n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1 = 0.901), metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = []\ny = []\nangle_data = []\nangle_num = []\nfor i in range(90):\n    angle_data = np.append(angle_data, dataset[i])\n    angle_num = np.append(angle_num, [i]*100000)\nangle_num = angle_num.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#n = len(dataset)\nimport random\ndef generate_real_samples(angle_data, angle_num, n_samples):\n\t# choose random instances\n\tix = randint(0, angle_data.shape[0], n_samples)\n\t# select images and labels\n\tangle_data, angle_num = angle_data[ix], angle_num[ix]\n\t# generate class labels\n\ty = ones((n_samples, 1))\n\treturn [angle_data, angle_num], y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate points in latent space as input for the generator\ndef generate_latent_points(latent_dim_gen, n, n_classes = 89):\n    # generate points in the latent space\n    x_input = randn(latent_dim_gen * n)\n    # reshape into a batch of inputs for the network\n    x_input = x_input.reshape(n, latent_dim_gen)\n    labels = randint(0, n_classes, n)\n    return [x_input, labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_fake_samples(generator, latent_dim_gen, n):\n        # generate points in latent space\n        data, angles = generate_latent_points(latent_dim_gen, n)\n        #print(data.shape)\n        #print(angles.shape)\n        # predict outputs\n        X = generator.predict([data, angles])\n        # create class labels\n        y = zeros((n, 1))\n        return [X,angles], y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(g_model, d_model, gan_model, latent_dim, latent_dim_gen, angle_data, angle_num, n_epochs=430, n_batch=256):\n        # determine half the size of one batch, for updating the discriminator\n        half_batch = int(n_batch / 2)\n        # manually enumerate epochs\n        for i in range(n_epochs):\n                # prepare real samples\n                [x_real,angle_real], y_real = generate_real_samples(angle_data, angle_num, half_batch)\n                # prepare fake examples\n                [x_fake,angle_fake], y_fake = generate_fake_samples(g_model, latent_dim_gen, half_batch)\n                # update discriminator\n                d_model.train_on_batch([x_real, angle_real], y_real)\n                d_model.train_on_batch([x_fake, angle_fake], y_fake)\n                # prepare points in latent space as input for the generator\n                [x_input, angle_input] = generate_latent_points(latent_dim_gen, n_batch)\n                # create inverted labels for the fake samples\n                y_gan = ones((n_batch, 1))\n                # update the generator via the discriminator's error\n                gan_model.train_on_batch([x_input, angle_input], y_gan)\n                # evaluate the model every n_eval epochs\n        gan_model.save('cgan_generator.h')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def real_training(angle_num, angle_data):\n    try:\n        os.mkdir(f'Gen-{angle_num[0]//9}')\n    except:\n        pass\n    latent_dim = 1\n    latent_dim_gen = 1\n    # create the discriminator\n    discriminator = define_discriminator(latent_dim)\n    # create the generator\n    generator = define_generator(latent_dim_gen)\n    # create the gan\n    gan_model = GAN_model(generator, discriminator)\n    # train model\n    train(generator, discriminator, gan_model, latent_dim, latent_dim_gen, angle_data, angle_num)\n    generator.save_weights(f'./Gen-{angle_num[0]}/generator-90')\n    print(f\"Training angle {angle_num[0]//9} done\", end='\\r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 90, 9):\n    angle_n = angle_num[i*angle_num.shape[0]//90: (i+9)*angle_num.shape[0]//90]\n    angle_d = angle_data[i*angle_num.shape[0]//90: (i+9)*angle_num.shape[0]//90]\n    real_training(angle_n, angle_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 4 #choosing angle 4\nangle_i_data = dataset[i]\ndata_hist = np.array(angle_i_data)\ndata_hist.shape\nplt.hist(data_hist, bins=300)\nplt.title('Column 85 - Env3 - Real')\nplt.savefig('Real-85-3.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_sample_space(latent_dim_gen, angle, n = 100000, n_classes = 10):\n    # generate points in the latent space\n    x_input = randn(latent_dim_gen * n)\n    # reshape into a batch of inputs for the network\n    x_input = x_input.reshape(n, latent_dim_gen)\n    labels = [angle] * n\n    labels = np.array(labels)\n    return [x_input, labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_samples(angle, latent_dim = 1, n=100000):\n    latent_pts, labels = generate_sample_space(latent_dim, angle)\n    #print(latent_pts.shape)\n    #print(labels.shape)\n    generator = define_generator(latent_dim)\n    generator.load_weights(f'./Gen-{angle//9}/generator-90')\n    X = generator.predict([latent_pts, labels])\n    y = zeros((n, 1))\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_fake, _ = generate_samples(4)\nx_fake = x_fake.reshape(100000,1)\nplt.hist(x_fake, bins=300)\nplt.title('Column 85 - Env3 - Generated')\nplt.savefig('Gen-85-3.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for angle in range(90):\n    angle_i_data = dataset[angle]\n    data_hist = np.array(angle_i_data)\n    print('='*10+f'angle-{angle+1}'+'='*10)\n    print(f'Real_Mean:{np.mean(data_hist)}; Real_Variance:{np.std(data_hist)**2}')\n    x_fake, _ = generate_samples(angle)\n    print(f'Fake_Mean:{np.mean(x_fake)}; Fake_Variance:{np.std(x_fake)**2}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" for angle in range(90):\n        x_fake, _ = generate_samples(angle)\n        x_fake = x_fake.reshape(100000,1)\n        plt.subplot(1,2,1)\n        plt.hist(x_fake, bins=300)\n        print(f'Angle:{angle}, Generated') \n        angle_i_data = dataset[angle]\n        data_hist = np.array(angle_i_data)\n        data_hist.shape\n        plt.subplot(1,2,2)\n        plt.hist(data_hist, bins=300)\n        print(f'Angle:{angle}, Real') \n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}